{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0c656e5-c47a-47fd-94c3-9a19087d3ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272a3e7e-58f0-4270-ba71-73613de3e5fc",
   "metadata": {},
   "source": [
    "# Target Variable\n",
    "## USGS Site AEK201\n",
    "Site [AEK201](https://cida.usgs.gov/ngwmn/provider/WAECY/site/100018881/) is a well monitored by the Washington State Department of Ecology, which makes data avaliable via their [Environmental Information Manament System](https://apps.ecology.wa.gov/eim/search/Eim/EIMSearchResults.aspx?ResultType=TimeSeriesLocationList&EIMSearchResultsFirstPageVisit=false&LocationSystemId=100018881&LocationUserIds=AEK201&LocationUserIdSearchType=Equals&LocationUserIDAliasSearchFlag=True). \n",
    "\n",
    "For our project, our goal is to forcast:\n",
    "- Water Levels, in feet below land surface\n",
    "\n",
    "### Importing and preparing the data\n",
    "From the raw data, we focus our attention on thee following columns:\n",
    "- `Field_Collection_Date_Time` - The date and time at which the Water Level measurment was recorded\n",
    "    - Reported in either PST (GMT-8) or PDT (GMT-7)\n",
    "- `Result_Value` - The Water Level (when `Result_Parameter_Name=='Water level in well (depth below measuring point)'`)\n",
    "    -  Measured in feet below the land surface\n",
    "\n",
    "Measurements are reported hourly.\n",
    "\n",
    "To ensure that our target data lines up with our feature data, we will group the measurments by day, and record their average as the 'well_depth'.\n",
    "\n",
    "To do this we:\n",
    "- Load the raw data\n",
    "- Restrict the data to rows where `Result_Parameter_Name=='Water level in well (depth below measuring point)'`\n",
    "- Restrict our attention to the `Field_Collection_Date_Time`, `Result_Value`, and `Time_Zone` columns\n",
    "- Construct a loalized timestame for each measurment and store it in a `datetime_recorded` column\n",
    "- Extract the `year`, `month`, and `day` as columns from the timestamp\n",
    "- Group measurments recorded on the same date, and compute their average as the daily `avg_well_depth`\n",
    "\n",
    "### The result\n",
    "\n",
    "The result is a dataframe called `level_data` with the following columns:\n",
    "- `date`\n",
    "- `avg_well_depth`\n",
    "\n",
    "which summarizes the average well depth measurment, in feet, for every day that we have data.\n",
    "\n",
    "\n",
    "# Important change!\n",
    "I've made some small edits to make it easier to run this notebook for other data sets; in particular, now we set the file paths for the raw well data and for the pickled data as variables that can be edited. There is a file in the repo with the relevant file paths.\n",
    "\n",
    "**You will need to check the names of your columns and of the `Result_Parameter_Name`, they might be different than here!!**\n",
    "\n",
    "Also, since the weather and surface water data has already been pickled, we don't need to run that cell again, though I've left it in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bef16070",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_well_data = '../data/raw_data/EIM-data-AEK201/EIMTimeSeriesResults_2023Oct22_222975.csv'\n",
    "final_pickle = '../data/pickled_data/AFL259_all_data.pkl'\n",
    "short_pickle = '../data/pickled_data/AFL259_short.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32953869-339b-4dfa-9881-d6b6a117b2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load raw data\n",
    "level_data = pd.read_csv(raw_well_data,\n",
    "                         low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95f9444b-09e7-407f-aa4d-dced45923810",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Restrict the data to rows where Result_Parameter_Name=='Water level in well (depth below measuring point)'\n",
    "level_data = level_data.loc[level_data['Result_Parameter_Name']=='Water level in well (depth below measuring point)']\n",
    "level_data = level_data.rename(columns={'Result_Value':'well_depth'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d03c5f39-8cdd-4ad7-9d62-eb0e859cd17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Restrict our attention to the Field_Collection_Date_Time, Result_Value, and Time_Zone columns\n",
    "level_data = level_data[['Field_Collection_Date_Time','well_depth','Time_Zone']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03e2c341-b0a0-4abe-a9b8-b6ef7c9021f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct a localized timestamp for each measurment and store it in a datetime_recorded column\n",
    "tz_dict = {'PDT - Pacific Daylight Time (GMT-7)':'Etc/GMT-7', \n",
    "           'PST - Pacific Standard Time (GMT-8)':'Etc/GMT-8'}\n",
    "\n",
    "level_data['Time_Zone']=level_data['Time_Zone'].apply(lambda x: tz_dict[x])\n",
    "\n",
    "level_data['Field_Collection_Date_Time'] = pd.to_datetime(\n",
    "    level_data['Field_Collection_Date_Time'], format = '%m/%d/%Y %H:%M:%S %p', utc=False)\n",
    "\n",
    "times = level_data.Field_Collection_Date_Time.values\n",
    "zones = level_data.Time_Zone.values\n",
    "localized_times = []\n",
    "for time, zone in zip(times, zones):\n",
    "    localized_times.append(pd.Timestamp(time).tz_localize(zone))\n",
    "\n",
    "level_data['datetime_recorded'] = localized_times\n",
    "\n",
    "## Sort by the timestamps\n",
    "level_data = level_data.sort_values('datetime_recorded')\n",
    "level_data = level_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9acb4883-0f3a-4355-9955-35f6fd62104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the date to a column\n",
    "level_data['date'] = level_data['datetime_recorded'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3df6a255-f49b-4b96-9839-b5755a047ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Group measurments recorded on the same date, and compute their average as the daily avg_well_depth\n",
    "level_data['avg_well_depth'] = level_data.groupby('date')['well_depth'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8651783-983c-4c08-811f-226de91290e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gather the columns we want, in the order we want\n",
    "level_data = level_data.drop_duplicates('date')[['date','avg_well_depth']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98015eb5-2b78-483e-88da-e24be6a812ba",
   "metadata": {},
   "source": [
    "# Feature Variables\n",
    "\n",
    "## Surface Water Data from the USGS\n",
    "\n",
    "USGS Site No: 12422500 [link](https://waterdata.usgs.gov/nwis/inventory?site_no=12422500)\n",
    "\n",
    "This site is reports the following data for the Spokane River in Spokane, WA:\n",
    "- Discharge, cubic feet per second (Mean)\n",
    "- Gage height, feet (Mean)\n",
    "\n",
    "### Importing and prepping\n",
    "- Load the raw data\n",
    "- Get the columns we want: `datetime_recorded`, `discharge_cfs`, and `gage_ht`)\n",
    "- Make the datatypes make sense\n",
    "- Extract the `year`, `month`, and `day` as columns from the timestamp\n",
    "- Break the data into two sets:\n",
    "    - Gage height:\n",
    "        - Restrict to 2005 and beyond\n",
    "        - Fill in missing values with the last non-missing value before the gap\n",
    "    - Discharge rate:\n",
    "        - Keep all the data\n",
    "     \n",
    "The result is two dataframes:\n",
    "- `sw_data_gage_ht` with the following columns:\n",
    "    - `date`\n",
    "    - `gage_ht`\n",
    "- `sw_data_discharge_cfs` with the following columns:\n",
    "    - `date`\n",
    "    - `discharge_cfs` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1888d129-c2e4-4ff0-bf45-e3137fa2285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the raw data\n",
    "sw_data = pd.read_csv('../data/raw_data/USGS-Surface-Water-Site-12422500.tsv',\n",
    "                      low_memory=False,\n",
    "                      delimiter='\\t',\n",
    "                      comment='#')\n",
    "\n",
    "## Drop meaningless top row\n",
    "sw_data = sw_data.drop(0, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "628a67ef-f371-4c26-aa70-736810205e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grab the columns we want\n",
    "sw_data = sw_data[['datetime','149640_00060_00003','149641_00065_00003']]\n",
    "\n",
    "## Rename the columns to something more meaningful\n",
    "headers = {'datetime':'datetime_recorded', '149640_00060_00003':'discharge_cfs', '149641_00065_00003':'gage_ht'}\n",
    "sw_data = sw_data.rename(columns=headers)\n",
    "\n",
    "## Make the column datatypes useful\n",
    "sw_data['datetime_recorded'] = pd.to_datetime(sw_data['datetime_recorded'])\n",
    "sw_data['discharge_cfs'] = sw_data['discharge_cfs'].astype(float)\n",
    "sw_data['gage_ht'] = sw_data['gage_ht'].astype(float)\n",
    "\n",
    "## Sort the data by the timestamp\n",
    "sw_data = sw_data.sort_values('datetime_recorded')\n",
    "sw_data = sw_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77641193-9703-448a-8201-b93630d0be64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the date as columns from the timestamp\n",
    "sw_data['date']=sw_data.datetime_recorded.dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2f883fe-0e85-40a0-8b85-e3c558e0ae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Restrict our attention to 2005 and beyond for the gage_ht\n",
    "sw_data_gage_ht = sw_data.loc[sw_data.datetime_recorded>=datetime(2005,1,1)][['date','gage_ht']].copy()\n",
    "## Fill missing gage_ht values with the last value before the gap\n",
    "sw_data_gage_ht = sw_data_gage_ht.fillna(method='ffill')\n",
    "\n",
    "## Keep all of the discharge data\n",
    "sw_data_discharge_cfs = sw_data[['date','discharge_cfs']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bd4b5b-5011-41a6-bb0f-a52925555cee",
   "metadata": {},
   "source": [
    "## Precipitation Data from NOAA\n",
    "\n",
    "Using data from the National Oceianic and Atmospheric Administration's (NOAA) National Centers for Environmental Information (NCEI) database, we ordered the following data from Spokane County, WA (FIPS:53063), for the dates 2005-01-01 through 2019-12-31.\n",
    "\n",
    "- EVAP - Evaporation of water from evaporation pan\n",
    "- DAPR - Number of days included in the multiday precipitation total (MDPR)\n",
    "- SX52 - Maximum soil temperature with sod cover at 10 cm depth\n",
    "- SNOW - Snowfall\n",
    "- WESF - Water equivalent of snowfall\n",
    "- PRCP - Precipitation\n",
    "- MXPN - Daily maximum temperature of water in an evaporation pan\n",
    "- SNWD - Snow depth\n",
    "- WESD - Water equivalent of snow on the ground\n",
    "- PSUN - Daily percent of possible sunshine for the period\n",
    "- MNPN - Daily minimum temperature of water in an evaporation pan\n",
    "- MDPR - Multiday precipitation total (use with DAPR and DWPR, if available)\n",
    "- SN52 - Minimum soil temperature with sod cover at 10 cm depth\n",
    "- TSUN - Total sunshine for the period\n",
    "\n",
    "Data is provided for several stations. we will use `USW00024157`, which is the weather station at the Spokane airport. We only want the precipitation (`PRCP`) information from this site.\n",
    "\n",
    "From the documentation, this data is provided as:\n",
    "> Precipitation - Total Liquid Content (TLC): Water equivalent amount of precipitation for the day (in inches to hundredths). This is all types of precipitation (melted and frozen). T indicates trace amount of precipitation. If left blank, precipitation amount is unreported.\n",
    "\n",
    "The result is a dataframe called `noaa_data` with the following columns:\n",
    "- `date`\n",
    "- `prcp`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dfa6600-967d-4f5e-a4d0-f375bd14d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "noaa_data = pd.read_csv('../data/raw_data/noaa-data.csv', parse_dates=['DATE'], low_memory=False)\n",
    "noaa_data['date']=noaa_data.DATE.dt.date\n",
    "noaa_data = noaa_data.loc[noaa_data.STATION=='USW00024157'][['date','PRCP']].copy()\n",
    "noaa_data = noaa_data.rename(columns={'PRCP':'prcp'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5547ea-399c-4f15-a4ff-3dc3a6721966",
   "metadata": {},
   "source": [
    "## Weather Data from Openweather.com\n",
    "\n",
    "Bulk weather history data is available for purchase [here](https://home.openweathermap.org/marketplace)\n",
    "\n",
    "For Spokane, WA, the following hourly measurments (starting in 1979) are available:\n",
    "- Temperature (Fahrenheit)\n",
    "- Min temperature (Fahrenheit)\n",
    "- Max temperature (Fahrenheit)\n",
    "- Feels like (Fahrenheit)\n",
    "- Pressure (hPa)\n",
    "- Humidity (%)\n",
    "- Clouds (%)\n",
    "- Weather conditions\n",
    "- Rain (mm/h)\n",
    "- Snow (mm/h)\n",
    "- Dew point (Fahrenheit)\n",
    "- Visibility (metres)\n",
    "- Wind (speed, direction, gust) (miles/hour, degrees, miles/hour)\n",
    "\n",
    "Of theses, we will be keeping:\n",
    "- Temperature (Fahrenheit)\n",
    "    - As the average daily temperature `temp_avg`, max daily temperature `temp_max`, and minimum daily temperature `temp_min`\n",
    "- Pressure (hPa)\n",
    "    - As the average daily pressure `hPa_avg` \n",
    "- Humidity (%)\n",
    "    - As the average daily `hum_avg`, max daily `hum_max`, min daily `hum_min`\n",
    "- Wind Speed (avg, max, min mph) `wind_avg`, `wind_max`, `wind_min`\n",
    "- Wind Gust (avg, max, min mph) `gust_avg`, `gust_max`, `gust_min`\n",
    "\n",
    "### Importing and Prepping\n",
    "- Import raw data\n",
    "- Create localized timestamps\n",
    "- Add a date column\n",
    "- Restrict to the columns of interest\n",
    "- Fill `NaN` with `0`\n",
    "- Compute `temp_avg`, `temp_max`, `temp_min`, `hPa_avg`, `hum_avg`, `hum_max`, `hum_min`, `wind_avg`, `wind_max`, `wind_min`, `gust_avg`, `gust_max`, and `gust_min`\n",
    "\n",
    "The result is a dataframed called `wx_data` with the following columns:\n",
    "- `date`\n",
    "- `temp_avg`\n",
    "- `temp_max`\n",
    "- `temp_min`\n",
    "- `hPa_avg`\n",
    "- `hum_avg`\n",
    "- `hum_max`\n",
    "- `hum_min`\n",
    "- `wind_avg`\n",
    "- `wind_max`\n",
    "- `wind_min`\n",
    "- `gust_avg`\n",
    "- `gust_max`\n",
    "- `gust_min`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "004e8aba-8dcb-4fa6-ab05-ad1a37bea59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import raw data\n",
    "wx_data = pd.read_csv('../data/raw_data/open-weather-spokane.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c886c07e-c0a1-4098-a567-8d40a0adc2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create localized timestamps\n",
    "def trunc(isodt):\n",
    "    return isodt[0:-10]\n",
    "\n",
    "wx_data['dt_iso'] = wx_data['dt_iso'].apply(trunc)\n",
    "\n",
    "wx_data['dt_iso'] = pd.to_datetime(wx_data['dt_iso'],\n",
    "                                       utc=True)\n",
    "wx_data['datetime_recorded'] = wx_data['dt_iso'].dt.tz_convert('US/Pacific')\n",
    "\n",
    "wx_data = wx_data.sort_values('datetime_recorded')\n",
    "wx_data = wx_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2946a42d-00bc-45a4-9988-6fcf4780dbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add a date column\n",
    "wx_data['date'] = wx_data.datetime_recorded.dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27a62a22-4f6f-4c2c-a283-bc86e9dc2204",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Restrict to the columns of interest\n",
    "wx_data = wx_data[['date',\n",
    "                   'temp',\n",
    "                   'pressure', \n",
    "                   'humidity', \n",
    "                   'wind_speed',\n",
    "                   'wind_gust']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d151c37-431c-431b-a3ac-4ce2e6a849dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill NaN values with zeros\n",
    "wx_data = wx_data.fillna(0)\n",
    "## Fix outliers\n",
    "wx_data.loc[287040,'temp']=10.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bb41470-1ec6-4efd-8a91-72152b645723",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compute the following:\n",
    "`temp_avg`, `temp_max`, `temp_min`, \n",
    "`hPa_avg`, \n",
    "`hum_avg`, `hum_max`, `hum_min`, \n",
    "`wind_avg`, `wind_max`, `wind_min`, \n",
    "`gust_avg`, `gust_max`, `gust_min`\n",
    "''' \n",
    "wx_data['temp_avg'] = wx_data.groupby('date')['temp'].transform('mean')\n",
    "wx_data['temp_max'] = wx_data.groupby('date')['temp'].transform('max')\n",
    "wx_data['temp_min'] = wx_data.groupby('date')['temp'].transform('min')\n",
    "wx_data['hPa_avg'] = wx_data.groupby('date')['pressure'].transform('mean')\n",
    "wx_data['hum_avg'] = wx_data.groupby('date')['humidity'].transform('mean')\n",
    "wx_data['hum_max'] = wx_data.groupby('date')['humidity'].transform('max')\n",
    "wx_data['hum_min'] = wx_data.groupby('date')['humidity'].transform('min')\n",
    "wx_data['wind_avg'] = wx_data.groupby('date')['wind_speed'].transform('mean')\n",
    "wx_data['wind_max'] = wx_data.groupby('date')['wind_speed'].transform('max')\n",
    "wx_data['wind_min'] = wx_data.groupby('date')['wind_speed'].transform('min')\n",
    "wx_data['gust_avg'] = wx_data.groupby('date')['wind_gust'].transform('mean')\n",
    "wx_data['gust_max'] = wx_data.groupby('date')['wind_gust'].transform('max')\n",
    "wx_data['gust_min'] = wx_data.groupby('date')['wind_gust'].transform('min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8389ad6-6725-473e-8701-ce999ed5588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wx_data = wx_data.drop_duplicates('date')[['date',\n",
    "                                               'temp_avg', 'temp_max', 'temp_min', \n",
    "                                               'hPa_avg',\n",
    "                                               'hum_avg', 'hum_max', 'hum_min',\n",
    "                                               'wind_avg', 'wind_max', 'wind_min', \n",
    "                                               'gust_avg', 'gust_max', 'gust_min']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba37556-fd07-4e1a-b1a2-28f1c75bda36",
   "metadata": {},
   "source": [
    "# Merging and Pickling\n",
    "After merging, the result is a dataframe called `all_data` with the following columns:\n",
    "- `date` - The date the measurements were recorded\n",
    "- `avg_well_depth` - The average of the daily well measurements, in feet from the surface\n",
    "- `gage_ht` - The gage height of the river, in feet\n",
    "- `discharge_cfs` - The discharge rate of the river in cubic feet per second\n",
    "- `temp_avg` - The average daily temperature in Fahrenheit\n",
    "- `temp_max` - The daily maximum temperature in Fahrenheit\n",
    "- `temp_min` - The daily minimum temperature in Fahrenheit\n",
    "- `hPa_avg` - The daily average pressure in hectopascals\n",
    "- `hum_avg` - The average daily humidity in percent\n",
    "- `hum_max` - The daily maximum humidity in percent\n",
    "- `hum_min` - The daily minimum humidity in percent\n",
    "- `prcp` - The daily precipitation total in inches\n",
    "- `wind_avg` - The average daily wind speed in miles per hour\n",
    "- `wind_max` - The daily maximum (hourly) wind speed in miles per hour\n",
    "- `wind_min` - The daily minimum (hourly) wind speed in miles per hour\n",
    "- `gust_avg` - The average daily wind gust speed in miles per hour\n",
    "- `gust_max` - The daily maximum wind gust speed in miles per hour\n",
    "- `gust_min` - The daily minimum wind gust speed in miles per hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c07c53fd-73a4-4543-a02c-d86ebc7faebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this doesn't need to be run again, these files have already\n",
    "# been created\n",
    "# changed to use the pandas to_pickle method\n",
    "level_data.to_pickle('../data/pickled_data/level_data.pkl')\n",
    "sw_data_gage_ht.to_pickle('../data/pickled_data/sw_data_gage_ht.pkl')\n",
    "sw_data_discharge_cfs.to_pickle('../data/pickled_data/sw_data_discharge_cfs.pkl')\n",
    "noaa_data.to_pickle('../data/pickled_data/noaa_data.pkl')\n",
    "wx_data.to_pickle('../data/pickled_data/wx_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "911698cf-f542-4b0f-b385-e630e2689c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = level_data.merge(sw_data_gage_ht, how='outer', on='date')\n",
    "all_data = all_data.merge(sw_data_discharge_cfs, how='outer', on='date')\n",
    "all_data = all_data.merge(noaa_data, how='outer', on='date')\n",
    "all_data = all_data.merge(wx_data, how='outer', on='date')\n",
    "all_data = all_data.sort_values('date')\n",
    "wx_data = wx_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b56c8c3-03c3-4401-ba0f-7d086336ebcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>avg_well_depth</th>\n",
       "      <th>gage_ht</th>\n",
       "      <th>discharge_cfs</th>\n",
       "      <th>prcp</th>\n",
       "      <th>temp_avg</th>\n",
       "      <th>temp_max</th>\n",
       "      <th>temp_min</th>\n",
       "      <th>hPa_avg</th>\n",
       "      <th>hum_avg</th>\n",
       "      <th>hum_max</th>\n",
       "      <th>hum_min</th>\n",
       "      <th>wind_avg</th>\n",
       "      <th>wind_max</th>\n",
       "      <th>wind_min</th>\n",
       "      <th>gust_avg</th>\n",
       "      <th>gust_max</th>\n",
       "      <th>gust_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6868</th>\n",
       "      <td>1900-10-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2410.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6869</th>\n",
       "      <td>1900-10-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2750.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6870</th>\n",
       "      <td>1900-10-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6871</th>\n",
       "      <td>1900-10-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3280.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6872</th>\n",
       "      <td>1900-10-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3460.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6863</th>\n",
       "      <td>2023-10-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.94</td>\n",
       "      <td>1470.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.655000</td>\n",
       "      <td>61.41</td>\n",
       "      <td>43.72</td>\n",
       "      <td>1020.958333</td>\n",
       "      <td>61.208333</td>\n",
       "      <td>80.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>9.831667</td>\n",
       "      <td>18.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6864</th>\n",
       "      <td>2023-10-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.95</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55.562083</td>\n",
       "      <td>72.59</td>\n",
       "      <td>42.53</td>\n",
       "      <td>1021.166667</td>\n",
       "      <td>58.708333</td>\n",
       "      <td>83.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>5.322917</td>\n",
       "      <td>10.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6865</th>\n",
       "      <td>2023-10-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.95</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.589167</td>\n",
       "      <td>76.69</td>\n",
       "      <td>42.44</td>\n",
       "      <td>1017.958333</td>\n",
       "      <td>62.375000</td>\n",
       "      <td>87.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>7.910417</td>\n",
       "      <td>16.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6866</th>\n",
       "      <td>2023-10-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.94</td>\n",
       "      <td>1470.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.808750</td>\n",
       "      <td>75.16</td>\n",
       "      <td>46.81</td>\n",
       "      <td>1015.375000</td>\n",
       "      <td>61.458333</td>\n",
       "      <td>76.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>6.279583</td>\n",
       "      <td>8.05</td>\n",
       "      <td>3.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6867</th>\n",
       "      <td>2023-10-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.95</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.523529</td>\n",
       "      <td>71.65</td>\n",
       "      <td>47.17</td>\n",
       "      <td>1012.647059</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>93.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5.753529</td>\n",
       "      <td>10.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44926 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  avg_well_depth  gage_ht  discharge_cfs  prcp   temp_avg  \\\n",
       "6868  1900-10-21             NaN      NaN         2410.0   NaN        NaN   \n",
       "6869  1900-10-22             NaN      NaN         2750.0   NaN        NaN   \n",
       "6870  1900-10-23             NaN      NaN         3100.0   NaN        NaN   \n",
       "6871  1900-10-24             NaN      NaN         3280.0   NaN        NaN   \n",
       "6872  1900-10-25             NaN      NaN         3460.0   NaN        NaN   \n",
       "...          ...             ...      ...            ...   ...        ...   \n",
       "6863  2023-10-17             NaN    17.94         1470.0   NaN  53.655000   \n",
       "6864  2023-10-18             NaN    17.95         1480.0   NaN  55.562083   \n",
       "6865  2023-10-19             NaN    17.95         1480.0   NaN  57.589167   \n",
       "6866  2023-10-20             NaN    17.94         1470.0   NaN  59.808750   \n",
       "6867  2023-10-21             NaN    17.95         1480.0   NaN  56.523529   \n",
       "\n",
       "      temp_max  temp_min      hPa_avg    hum_avg  hum_max  hum_min  wind_avg  \\\n",
       "6868       NaN       NaN          NaN        NaN      NaN      NaN       NaN   \n",
       "6869       NaN       NaN          NaN        NaN      NaN      NaN       NaN   \n",
       "6870       NaN       NaN          NaN        NaN      NaN      NaN       NaN   \n",
       "6871       NaN       NaN          NaN        NaN      NaN      NaN       NaN   \n",
       "6872       NaN       NaN          NaN        NaN      NaN      NaN       NaN   \n",
       "...        ...       ...          ...        ...      ...      ...       ...   \n",
       "6863     61.41     43.72  1020.958333  61.208333     80.0     40.0  9.831667   \n",
       "6864     72.59     42.53  1021.166667  58.708333     83.0     28.0  5.322917   \n",
       "6865     76.69     42.44  1017.958333  62.375000     87.0     34.0  7.910417   \n",
       "6866     75.16     46.81  1015.375000  61.458333     76.0     40.0  6.279583   \n",
       "6867     71.65     47.17  1012.647059  72.000000     93.0     43.0  5.753529   \n",
       "\n",
       "      wind_max  wind_min  gust_avg  gust_max  gust_min  \n",
       "6868       NaN       NaN       NaN       NaN       NaN  \n",
       "6869       NaN       NaN       NaN       NaN       NaN  \n",
       "6870       NaN       NaN       NaN       NaN       NaN  \n",
       "6871       NaN       NaN       NaN       NaN       NaN  \n",
       "6872       NaN       NaN       NaN       NaN       NaN  \n",
       "...        ...       ...       ...       ...       ...  \n",
       "6863     18.41      0.00       0.0       0.0       0.0  \n",
       "6864     10.36      0.00       0.0       0.0       0.0  \n",
       "6865     16.11      0.00       0.0       0.0       0.0  \n",
       "6866      8.05      3.44       0.0       0.0       0.0  \n",
       "6867     10.36      0.00       0.0       0.0       0.0  \n",
       "\n",
       "[44926 rows x 18 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e34a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a date to datetime fix\n",
    "\n",
    "## Make the dates `nice`:\n",
    "## check the first and last date from above and make sure the number of\n",
    "## days matches so there are no missing days\n",
    "date_rng = pd.date_range(start='1900-10-21', end='2023-10-21', freq='D')\n",
    "all_data['date'] = date_rng\n",
    "\n",
    "## Make the index `nice`:\n",
    "all_data = all_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4d0da67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncating the data to the relevant dates\n",
    "# this may depend on the well, but I'm relatively sure that\n",
    "# these dates are included for all of our wells of interest\n",
    "min_date = datetime(2006,2,7)\n",
    "max_date = datetime(2017,9,28)\n",
    "df_short = all_data.loc[(all_data.date >= min_date) & (all_data.date <= max_date)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb42929a-2021-44fa-af0b-bc15a6c2986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pickle the full data and the short version\n",
    "all_data.to_pickle(final_pickle)\n",
    "df_short.to_pickle(short_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac49f90-0bd1-486a-8c0c-798eff6c15b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0c656e5-c47a-47fd-94c3-9a19087d3ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272a3e7e-58f0-4270-ba71-73613de3e5fc",
   "metadata": {},
   "source": [
    "# Target Variable\n",
    "## USGS Site AEK201\n",
    "Site [AEK201](https://cida.usgs.gov/ngwmn/provider/WAECY/site/100018881/) is a well monitored by the Washington State Department of Ecology, which makes data avaliable via their [Environmental Information Manament System](https://apps.ecology.wa.gov/eim/search/Eim/EIMSearchResults.aspx?ResultType=TimeSeriesLocationList&EIMSearchResultsFirstPageVisit=false&LocationSystemId=100018881&LocationUserIds=AEK201&LocationUserIdSearchType=Equals&LocationUserIDAliasSearchFlag=True). \n",
    "\n",
    "For our project, our goal is to forcast:\n",
    "- Water Levels, in feet below land surface\n",
    "\n",
    "### Importing and preparing the data\n",
    "From the raw data, we focus our attention on thee following columns:\n",
    "- `Field_Collection_Date_Time` - The date and time at which the Water Level measurment was recorded\n",
    "    - Reported in either PST (GMT-8) or PDT (GMT-7)\n",
    "- `Result_Value` - The Water Level (when `Result_Parameter_Name=='Water level in well (depth below measuring point)'`)\n",
    "    -  Measured in feet below the land surface\n",
    "\n",
    "Measurements are reported hourly.\n",
    "\n",
    "To ensure that our target data lines up with our feature data, we will group the measurments by day, and record their average as the 'well_depth'.\n",
    "\n",
    "To do this we:\n",
    "- Load the raw data\n",
    "- Restrict the data to rows where `Result_Parameter_Name=='Water level in well (depth below measuring point)'`\n",
    "- Restrict our attention to the `Field_Collection_Date_Time`, `Result_Value`, and `Time_Zone` columns\n",
    "- Construct a loalized timestame for each measurment and store it in a `datetime_recorded` column\n",
    "- Extract the `year`, `month`, and `day` as columns from the timestamp\n",
    "- Group measurments recorded on the same date, and compute their average as the daily `avg_well_depth`\n",
    "\n",
    "### The result\n",
    "\n",
    "The result is a dataframe called `level_data` with the following columns:\n",
    "- `date`\n",
    "- `avg_well_depth`\n",
    "\n",
    "which summarizes the average well depth measurment, in feet, for every day that we have data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32953869-339b-4dfa-9881-d6b6a117b2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load raw data\n",
    "level_data = pd.read_csv('../data-collection/data/EIM-data-AEK201/EIMTimeSeriesResults_2023Oct22_222975.csv',\n",
    "                         low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95f9444b-09e7-407f-aa4d-dced45923810",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Restrict the data to rows where Result_Parameter_Name=='Water level in well (depth below measuring point)'\n",
    "level_data = level_data.loc[level_data['Result_Parameter_Name']=='Water level in well (depth below measuring point)']\n",
    "level_data = level_data.rename(columns={'Result_Value':'well_depth'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d03c5f39-8cdd-4ad7-9d62-eb0e859cd17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Restrict our attention to the Field_Collection_Date_Time, Result_Value, and Time_Zone columns\n",
    "level_data = level_data[['Field_Collection_Date_Time','well_depth','Time_Zone']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03e2c341-b0a0-4abe-a9b8-b6ef7c9021f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct a localized timestame for each measurment and store it in a datetime_recorded column\n",
    "tz_dict = {'PDT - Pacific Daylight Time (GMT-7)':'Etc/GMT-7', \n",
    "           'PST - Pacific Standard Time (GMT-8)':'Etc/GMT-8'}\n",
    "\n",
    "level_data['Time_Zone']=level_data['Time_Zone'].apply(lambda x: tz_dict[x])\n",
    "\n",
    "level_data['Field_Collection_Date_Time'] = pd.to_datetime(\n",
    "    level_data['Field_Collection_Date_Time'], format = '%m/%d/%Y %H:%M:%S %p', utc=False)\n",
    "\n",
    "times = level_data.Field_Collection_Date_Time.values\n",
    "zones = level_data.Time_Zone.values\n",
    "localized_times = []\n",
    "for time, zone in zip(times, zones):\n",
    "    localized_times.append(pd.Timestamp(time).tz_localize(zone))\n",
    "\n",
    "level_data['datetime_recorded'] = localized_times\n",
    "\n",
    "## Sort by the timestamps\n",
    "level_data = level_data.sort_values('datetime_recorded')\n",
    "level_data = level_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9acb4883-0f3a-4355-9955-35f6fd62104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the date to a column\n",
    "level_data['date'] = level_data['datetime_recorded'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3df6a255-f49b-4b96-9839-b5755a047ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Group measurments recorded on the same date, and compute their average as the daily avg_well_depth\n",
    "level_data['avg_well_depth'] = level_data.groupby('date')['well_depth'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8651783-983c-4c08-811f-226de91290e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gather the columns we want, in the order we want\n",
    "level_data = level_data.drop_duplicates('date')[['date','avg_well_depth']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98015eb5-2b78-483e-88da-e24be6a812ba",
   "metadata": {},
   "source": [
    "# Feature Variables\n",
    "\n",
    "## Surface Water Data from the USGS\n",
    "\n",
    "USGS Site No: 12422500 [link](https://waterdata.usgs.gov/nwis/inventory?site_no=12422500)\n",
    "\n",
    "This site is reports the following data for the Spokane River in Spokane, WA:\n",
    "- Discharge, cubic feet per second (Mean)\n",
    "- Gage height, feet (Mean)\n",
    "\n",
    "### Importing and prepping\n",
    "- Load the raw data\n",
    "- Get the columns we want: `datetime_recorded`, `discharge_cfs`, and `gage_ht`)\n",
    "- Make the datatypes make sense\n",
    "- Extract the `year`, `month`, and `day` as columns from the timestamp\n",
    "- Break the data into two sets:\n",
    "    - Gage height:\n",
    "        - Restrict to 2005 and beyond\n",
    "        - Fill in missing values with the last non-missing value before the gap\n",
    "    - Discharge rate:\n",
    "        - Keep all the data\n",
    "     \n",
    "The result is a two dataframes:\n",
    "- `sw_data_gage_ht` with the following columns:\n",
    "    - `date`\n",
    "    - `gage_ht`\n",
    "- `sw_data_discharge_cfs` with the following columns:\n",
    "    - `date`\n",
    "    - `discharge_cfs` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1888d129-c2e4-4ff0-bf45-e3137fa2285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the raw data\n",
    "sw_data = pd.read_csv('../data-collection/data/USGS-Surface-Water-Site-12422500.tsv',\n",
    "                      low_memory=False,\n",
    "                      delimiter='\\t',\n",
    "                      comment='#')\n",
    "\n",
    "## Drop meaningless top row\n",
    "sw_data = sw_data.drop(0, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "628a67ef-f371-4c26-aa70-736810205e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grab the columns we want\n",
    "sw_data = sw_data[['datetime','149640_00060_00003','149641_00065_00003']]\n",
    "\n",
    "## Rename the columns to something more meaningful\n",
    "headers = {'datetime':'datetime_recorded', '149640_00060_00003':'discharge_cfs', '149641_00065_00003':'gage_ht'}\n",
    "sw_data = sw_data.rename(columns=headers)\n",
    "\n",
    "## Make the column datatypes useful\n",
    "sw_data['datetime_recorded'] = pd.to_datetime(sw_data['datetime_recorded'])\n",
    "sw_data['discharge_cfs'] = sw_data['discharge_cfs'].astype(float)\n",
    "sw_data['gage_ht'] = sw_data['gage_ht'].astype(float)\n",
    "\n",
    "## Sort the data by the timestamp\n",
    "sw_data = sw_data.sort_values('datetime_recorded')\n",
    "sw_data = sw_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77641193-9703-448a-8201-b93630d0be64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the date as columns from the timestamp\n",
    "sw_data['date']=sw_data.datetime_recorded.dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2f883fe-0e85-40a0-8b85-e3c558e0ae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Restrict our attention to 2005 and beyond for the gage_ht\n",
    "sw_data_gage_ht = sw_data.loc[sw_data.datetime_recorded>=datetime(2005,1,1)][['date','gage_ht']].copy()\n",
    "## Fill missing gage_ht values with the last value before the gap\n",
    "sw_data_gage_ht = sw_data_gage_ht.fillna(method='ffill')\n",
    "\n",
    "## Keep all of the discharge data\n",
    "sw_data_discharge_cfs = sw_data[['date','discharge_cfs']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5547ea-399c-4f15-a4ff-3dc3a6721966",
   "metadata": {},
   "source": [
    "## Weather Data from Openweather.com\n",
    "\n",
    "Bulk weather history data is available for purchase [here](https://home.openweathermap.org/marketplace)\n",
    "\n",
    "For Spokane, WA, the following hourly measurments (starting in 1979) are available:\n",
    "- Temperature (Fahrenheit)\n",
    "- Min temperature (Fahrenheit)\n",
    "- Max temperature (Fahrenheit)\n",
    "- Feels like (Fahrenheit)\n",
    "- Pressure (hPa)\n",
    "- Humidity (%)\n",
    "- Clouds (%)\n",
    "- Weather conditions\n",
    "- Rain (mm/h)\n",
    "- Snow (mm/h)\n",
    "- Dew point (Fahrenheit)\n",
    "- Visibility (metres)\n",
    "- Wind (speed, direction, gust) (miles/hour, degrees, miles/hour)\n",
    "\n",
    "Of theses, we will be keeping:\n",
    "- Temperature (Fahrenheit)\n",
    "    - As the average daily temperature `temp_avg`, max daily temperature `temp_max`, and minimum daily temperature `temp_min`\n",
    "- Pressure (hPa)\n",
    "    - As the average daily pressure `hPa_avg` \n",
    "- Humidity (%)\n",
    "    - As the average daily `hum_avg`, max daily `hum_max`, min daily `hum_min`\n",
    "- Rain (mm/h)\n",
    "    - As the cumulative daily rain total in mm `rain` calulcated based the hourly rain reoported\n",
    "- Snow (mm/h)\n",
    "    - As the cumulative daily rain total in mm `snow` calulcated based the hourly rain reoported\n",
    "- Wind Speed (avg, max, min mph) `wind_avg`, `wind_max`, `wind_min`\n",
    "- Wind Gust (avg, max, min mph) `gust_avg`, `gust_max`, `gust_min`\n",
    "\n",
    "### Importing and Prepping\n",
    "- Import raw data\n",
    "- Create localized timestamps\n",
    "- Add a date column\n",
    "- Restrict to the columns of interest\n",
    "- Fill `NaN` with `0`\n",
    "- Compute `temp_avg`, `temp_max`, `temp_min`, `hPa_avg`, `hum_avg`, `hum_max`, `hum_min`, `rain`, `snow`, `wind_avg`, `wind_max`, `wind_min`, `gust_avg`, `gust_max`, and `gust_min`\n",
    "\n",
    "The result is a dataframed called `wx_data` with the following columns:\n",
    "- `date`\n",
    "- `temp_avg`\n",
    "- `temp_max`\n",
    "- `temp_min`\n",
    "- `hPa_avg`\n",
    "- `hum_avg`\n",
    "- `hum_max`\n",
    "- `hum_min`\n",
    "- `rain`\n",
    "- `snow`\n",
    "- `wind_avg`\n",
    "- `wind_max`\n",
    "- `wind_min`\n",
    "- `gust_avg`\n",
    "- `gust_max`\n",
    "- `gust_min`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "004e8aba-8dcb-4fa6-ab05-ad1a37bea59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import raw data\n",
    "wx_data = pd.read_csv('../data-collection/data/open-weather-spokane.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c886c07e-c0a1-4098-a567-8d40a0adc2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create localized timestamps\n",
    "def trunc(isodt):\n",
    "    return isodt[0:-10]\n",
    "\n",
    "wx_data['dt_iso'] = wx_data['dt_iso'].apply(trunc)\n",
    "\n",
    "wx_data['dt_iso'] = pd.to_datetime(wx_data['dt_iso'],\n",
    "                                       utc=True)\n",
    "wx_data['datetime_recorded'] = wx_data['dt_iso'].dt.tz_convert('US/Pacific')\n",
    "\n",
    "wx_data = wx_data.sort_values('datetime_recorded')\n",
    "wx_data = wx_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2946a42d-00bc-45a4-9988-6fcf4780dbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add a date column\n",
    "wx_data['date'] = wx_data.datetime_recorded.dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27a62a22-4f6f-4c2c-a283-bc86e9dc2204",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Restrict to the columns of interest\n",
    "wx_data = wx_data[['date',\n",
    "                   'temp',\n",
    "                   'pressure', \n",
    "                   'humidity', \n",
    "                   'wind_speed',\n",
    "                   'wind_gust', \n",
    "                   'rain_1h',\n",
    "                   'snow_1h',]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d151c37-431c-431b-a3ac-4ce2e6a849dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill NaN values with zeros\n",
    "wx_data = wx_data.fillna(0)\n",
    "## Fix outliers\n",
    "wx_data.loc[287040,'temp']=10.09\n",
    "##\n",
    "wx_data.loc[134923,'rain_1h']=0\n",
    "wx_data.loc[134924,'rain_1h']=0\n",
    "wx_data.loc[375773,'rain_1h']=0\n",
    "wx_data.loc[398946,'rain_1h']=0\n",
    "wx_data.loc[403544,'rain_1h']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bb41470-1ec6-4efd-8a91-72152b645723",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compute the following:\n",
    "`temp_avg`, `temp_max`, `temp_min`, \n",
    "`hPa_avg`, \n",
    "`hum_avg`, `hum_max`, `hum_min`, \n",
    "`rain`, \n",
    "`snow`, \n",
    "`wind_avg`, `wind_max`, `wind_min`, \n",
    "`gust_avg`, `gust_max`, `gust_min`\n",
    "''' \n",
    "wx_data['temp_avg'] = wx_data.groupby('date')['temp'].transform('mean')\n",
    "wx_data['temp_max'] = wx_data.groupby('date')['temp'].transform('max')\n",
    "wx_data['temp_min'] = wx_data.groupby('date')['temp'].transform('min')\n",
    "wx_data['hPa_avg'] = wx_data.groupby('date')['pressure'].transform('mean')\n",
    "wx_data['hum_avg'] = wx_data.groupby('date')['humidity'].transform('mean')\n",
    "wx_data['hum_max'] = wx_data.groupby('date')['humidity'].transform('max')\n",
    "wx_data['hum_min'] = wx_data.groupby('date')['humidity'].transform('min')\n",
    "wx_data['rain'] = wx_data.groupby('date')['rain_1h'].transform('sum')\n",
    "wx_data['snow'] = wx_data.groupby('date')['snow_1h'].transform('sum')\n",
    "wx_data['wind_avg'] = wx_data.groupby('date')['wind_speed'].transform('mean')\n",
    "wx_data['wind_max'] = wx_data.groupby('date')['wind_speed'].transform('max')\n",
    "wx_data['wind_min'] = wx_data.groupby('date')['wind_speed'].transform('min')\n",
    "wx_data['gust_avg'] = wx_data.groupby('date')['wind_gust'].transform('mean')\n",
    "wx_data['gust_max'] = wx_data.groupby('date')['wind_gust'].transform('max')\n",
    "wx_data['gust_min'] = wx_data.groupby('date')['wind_gust'].transform('min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8389ad6-6725-473e-8701-ce999ed5588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wx_data = wx_data.drop_duplicates('date')[['date',\n",
    "                                               'temp_avg', 'temp_max', 'temp_min', \n",
    "                                               'hPa_avg',\n",
    "                                               'hum_avg', 'hum_max', 'hum_min', \n",
    "                                               'rain', \n",
    "                                               'snow', \n",
    "                                               'wind_avg', 'wind_max', 'wind_min', \n",
    "                                               'gust_avg', 'gust_max', 'gust_min']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba37556-fd07-4e1a-b1a2-28f1c75bda36",
   "metadata": {},
   "source": [
    "# Merging and Pickling\n",
    "After merging, the result is a dataframe called `all_data` with the following columns:\n",
    "- `date` - The date the measurements were recorded\n",
    "- `avg_well_depth` - The average of the daily well measurements, in feet from the surface\n",
    "- `gage_ht` - The gage height of the river, in feet\n",
    "- `discharge_cfs` - The discharge rate of the river in cubic feet per second\n",
    "- `temp_avg` - The average daily temperature in Fahrenheit\n",
    "- `temp_max` - The daily maximum temperature in Fahrenheit\n",
    "- `temp_min` - The daily minimum temperature in Fahrenheit\n",
    "- `hPa_avg` - The daily average pressure in hectopascals\n",
    "- `hum_avg` - The average daily humidity in percent\n",
    "- `hum_max` - The daily maximum humidity in percent\n",
    "- `hum_min` - The daily minimum humidity in percent\n",
    "- `rain` - The daily rain total in millimeters\n",
    "- `snow` - The daily snow total in millimeters\n",
    "- `wind_avg` - The average daily wind speed in miles per hour\n",
    "- `wind_max` - The daily maximum (hourly) wind speed in miles per hour\n",
    "- `wind_min` - The daily minimum (hourly) wind speed in miles per hour\n",
    "- `gust_avg` - The average daily wind gust speed in miles per hour\n",
    "- `gust_max` - The daily maximum wind gust speed in miles per hour\n",
    "- `gust_min` - The daily minimum wind gust speed in miles per hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c07c53fd-73a4-4543-a02c-d86ebc7faebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data-collection/data/level_data.pkl', 'wb') as f:\n",
    "    pickle.dump(level_data, f)\n",
    "with open('../data-collection/data/sw_data_gage_ht.pkl', 'wb') as f:\n",
    "    pickle.dump(sw_data_gage_ht, f)\n",
    "with open('../data-collection/data/sw_data_discharge_cfs.pkl', 'wb') as f:\n",
    "    pickle.dump(sw_data_discharge_cfs, f)\n",
    "with open('../data-collection/data/wx_data.pkl', 'wb') as f:\n",
    "    pickle.dump(wx_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "911698cf-f542-4b0f-b385-e630e2689c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = level_data.merge(sw_data_gage_ht, how='outer', on='date')\n",
    "all_data = all_data.merge(sw_data_discharge_cfs, how='outer', on='date')\n",
    "all_data = all_data.merge(wx_data, how='outer', on='date')\n",
    "all_data = all_data.sort_values('date')\n",
    "wx_data = wx_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb42929a-2021-44fa-af0b-bc15a6c2986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data-collection/data/all_data.pkl', 'wb') as f:\n",
    "    pickle.dump(all_data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (data-science)",
   "language": "python",
   "name": "data-science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9cee0ea-d711-4b64-9fa2-4617788ab74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from gw_tools import data_prep\n",
    "from gw_tools.gw_cnn import gw_cnn\n",
    "from gw_tools.gw_LSTM import gw_LSTM\n",
    "from gw_tools import model_params\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea302c20-ef06-4d79-9cb4-1e88e5986bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_path = '../data/prediction_pickles/'\n",
    "score_path = '../data/prediction_pickles/'\n",
    "future_pred_path = '../data/prediction_pickles/'\n",
    "future_score_path = '../data/prediction_pickles/'\n",
    "\n",
    "\n",
    "score_summary = pd.DataFrame({'Model':['Baseline', 'Linear Regression', 'CNN', 'LSTM'],'RMSE':[0,0,0,0], 'MAE':[0,0,0,0]})\n",
    "score_summary_future = pd.DataFrame({'Model':['Baseline', 'Linear Regression', 'CNN', 'LSTM'],'RMSE':[0,0,0,0], 'MAE':[0,0,0,0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1222de19-6912-4dd2-856d-1646610c8083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-24 14:26:14.095193: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Max\n",
      "2023-11-24 14:26:14.095209: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 64.00 GB\n",
      "2023-11-24 14:26:14.095213: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 24.00 GB\n",
      "2023-11-24 14:26:14.095248: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-11-24 14:26:14.095261: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-24 14:26:14.593960: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 1s 7ms/step - loss: 742.7554\n",
      "Epoch 2/100\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 75.8852\n",
      "Epoch 3/100\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 9.2528\n",
      "Epoch 4/100\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 3.0244\n",
      "Epoch 5/100\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 2.4874\n",
      "Epoch 6/100\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 1.2998\n",
      "Epoch 7/100\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 1.1162\n",
      "Epoch 8/100\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.9761\n",
      "Epoch 9/100\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.8007\n",
      "Epoch 10/100\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.7320\n",
      "Epoch 11/100\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.7172\n",
      "Epoch 12/100\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.7376\n",
      "Epoch 13/100\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.5462\n",
      "Epoch 14/100\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.6887\n",
      "Epoch 15/100\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.8378\n",
      "Epoch 16/100\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.9876\n",
      "Epoch 17/100\n",
      "124/124 [==============================] - 1s 6ms/step - loss: 0.5820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-24 14:26:28.548440: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-11-24 14:26:28.634410: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-11-24 14:26:28.733915: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/123 [==============================] - 1s 7ms/step - loss: 2105.1531 - root_mean_squared_error: 45.8819\n",
      "Epoch 2/30\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 11.8948 - root_mean_squared_error: 3.4489\n",
      "Epoch 3/30\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 8.6973 - root_mean_squared_error: 2.9491\n",
      "Epoch 4/30\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 6.5593 - root_mean_squared_error: 2.5611\n",
      "Epoch 5/30\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 2.3409 - root_mean_squared_error: 1.5300\n",
      "Epoch 6/30\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.3901 - root_mean_squared_error: 0.6246\n",
      "Epoch 7/30\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.1688 - root_mean_squared_error: 0.4109\n",
      "Epoch 8/30\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.1165 - root_mean_squared_error: 0.3413\n",
      "Epoch 9/30\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0934 - root_mean_squared_error: 0.3055\n",
      "Epoch 10/30\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.0777 - root_mean_squared_error: 0.2788\n",
      "Epoch 11/30\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.0663 - root_mean_squared_error: 0.2576\n",
      "Epoch 12/30\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0589 - root_mean_squared_error: 0.2427\n",
      "Epoch 13/30\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0522 - root_mean_squared_error: 0.2285\n",
      "Epoch 14/30\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0463 - root_mean_squared_error: 0.2153\n",
      "Epoch 15/30\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0421 - root_mean_squared_error: 0.2053\n",
      "Epoch 16/30\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.0393 - root_mean_squared_error: 0.1983\n",
      "Epoch 17/30\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0359 - root_mean_squared_error: 0.1894\n",
      "Epoch 18/30\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.0335 - root_mean_squared_error: 0.1829\n",
      "Epoch 19/30\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.0309 - root_mean_squared_error: 0.1757\n",
      "Epoch 20/30\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.0278 - root_mean_squared_error: 0.1668\n",
      "Epoch 21/30\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.0254 - root_mean_squared_error: 0.1592\n",
      "Epoch 22/30\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0240 - root_mean_squared_error: 0.1548\n",
      "Epoch 23/30\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.0226 - root_mean_squared_error: 0.1503\n",
      "Epoch 24/30\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0219 - root_mean_squared_error: 0.1479\n",
      "Epoch 25/30\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0219 - root_mean_squared_error: 0.1481\n",
      "Epoch 26/30\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0205 - root_mean_squared_error: 0.1432\n",
      "Epoch 27/30\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0204 - root_mean_squared_error: 0.1429\n",
      "Epoch 28/30\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0202 - root_mean_squared_error: 0.1421\n",
      "Epoch 29/30\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0195 - root_mean_squared_error: 0.1396\n",
      "Epoch 30/30\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0192 - root_mean_squared_error: 0.1385\n",
      " 42/124 [=========>....................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-24 14:26:53.124201: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 5ms/step\n",
      "70/70 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 84\u001b[0m\n\u001b[1;32m     78\u001b[0m pred_cnn_full \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(pred_cnn_train, \n\u001b[1;32m     79\u001b[0m                           np\u001b[38;5;241m.\u001b[39mappend(pred_cnn_test, pred_cnn_future))\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m## Make LSTM prediction\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# the LSTM requires a warmup set, so the first window=WINDOW_SIZE predictions for the training\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# set will just be the actual values\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m ytr_warmup \u001b[38;5;241m=\u001b[39m \u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m()\n\u001b[1;32m     85\u001b[0m pred_LSTM_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((ytr_warmup, pipe_LSTM\u001b[38;5;241m.\u001b[39mpredict(X_tr_L)))\n\u001b[1;32m     86\u001b[0m pred_LSTM_test \u001b[38;5;241m=\u001b[39m pipe_LSTM\u001b[38;5;241m.\u001b[39mpredict(X_te_L)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_numpy'"
     ]
    }
   ],
   "source": [
    "## Define a scaler\n",
    "scaler_cnn =  StandardScaler(copy=True)\n",
    "scaler_lstm =  StandardScaler(copy=True)\n",
    "\n",
    "\n",
    "## note: parameters are stored in model_params, imported above\n",
    "for well, params, LSTMparams in zip(model_params.wells, \n",
    "                                    model_params.CNN_well_params, \n",
    "                                    model_params.LSTM_well_params):\n",
    "    ## Prepare the data for training\n",
    "    df = data_prep.load_data(well)\n",
    "    well_dates = df['date'].copy()\n",
    "    df = data_prep.select_features(df)\n",
    "    df = data_prep.add_toy_signal(df)\n",
    "    \n",
    "    ## LSTM specific prep (requires a warmup, target as a feature)\n",
    "    # we also need the window size later for the future predictions\n",
    "    window = LSTMparams['model__WINDOW_SIZE']\n",
    "    X_tr_L, X_te_L, well_tr_mean, well_tr_std = data_prep.LSTM_data_prep(df,window)\n",
    "    # Set the mean and standard deviation of the training set as parameters\n",
    "    LSTMparams['model__tmean'] = well_tr_mean\n",
    "    LSTMparams['model__tsd'] = well_tr_std\n",
    "    \n",
    "    \n",
    "    ## CNN data\n",
    "    X_train, X_holdout, y_train, y_holdout, dt_train, dt_holdout = data_prep.prep_data_for_training(df)\n",
    "    \n",
    "    \n",
    "    ## Get average for baseline prediction\n",
    "    train_mean = y_train.mean()\n",
    "    \n",
    "    ## Train the linear regression model\n",
    "    model_lr = LinearRegression(copy_X=True)\n",
    "    model_lr.fit(X_train, y_train)\n",
    "    \n",
    "    ## Train the CNN\n",
    "    model_cnn = gw_cnn()\n",
    "    pipe_cnn = Pipeline([('scaler', scaler_cnn), ('model', model_cnn)])\n",
    "    pipe_cnn.set_params(**params)\n",
    "    pipe_cnn.fit(X_train, y_train)\n",
    "\n",
    "    ## Train the LSTM\n",
    "    model_LSTM = gw_LSTM()\n",
    "    pipe_LSTM = Pipeline([('scaler', scaler_lstm), ('model', model_LSTM)])\n",
    "    pipe_LSTM.set_params(**LSTMparams)\n",
    "    pipe_LSTM.fit(X_tr_L, y_train)\n",
    "\n",
    "    \n",
    "    ## Prepare future data for predictions and scoring\n",
    "    features = data_prep.load_data('FEATS')\n",
    "    features = data_prep.select_features(features, no_target=True)\n",
    "    features = data_prep.add_toy_signal(features)\n",
    "    future_dates = features.loc[features.date > data_prep.get_end_date(well)]['date'].copy()\n",
    "    # CNN specific future data\n",
    "    X_future = features.loc[features.date > data_prep.get_end_date(well)].drop('date', axis=1).copy()\n",
    "    all_dates = pd.concat([well_dates, future_dates])\n",
    "    \n",
    "    ## Keep track of the length of predictions\n",
    "    len_pred_test = len(y_holdout)\n",
    "    len_pred_future = all_dates.shape[0]-len_pred_test\n",
    "    \n",
    "    ## Make baseline predictions\n",
    "    pred_avg_train = np.ones(len_pred_test)*train_mean\n",
    "    pred_avg_future = np.ones(len_pred_future)*train_mean\n",
    "    pred_avg_full = np.append(pred_avg_train, pred_avg_future)\n",
    "    \n",
    "    ## Make linear regression predictions\n",
    "    pred_lin_train = model_lr.predict(X_train)\n",
    "    pred_lin_test = model_lr.predict(X_holdout)\n",
    "    pred_lin_future = model_lr.predict(X_future.values)\n",
    "    pred_lin_full = np.append(pred_lin_train, \n",
    "                              np.append(pred_lin_test, pred_lin_future))\n",
    "    \n",
    "    ## Make CNN predictions\n",
    "    pred_cnn_train = pipe_cnn.predict(X_train)\n",
    "    pred_cnn_test = pipe_cnn.predict(X_holdout)\n",
    "    pred_cnn_future = pipe_cnn.predict(X_future.values)\n",
    "    pred_cnn_full = np.append(pred_cnn_train, \n",
    "                              np.append(pred_cnn_test, pred_cnn_future))\n",
    "\n",
    "    ## Make LSTM prediction\n",
    "    # the LSTM requires a warmup set, so the first window=WINDOW_SIZE predictions for the training\n",
    "    # set will just be the actual values\n",
    "    ytr_warmup = y_train[:window].to_numpy()\n",
    "    pred_LSTM_train = np.concatenate((ytr_warmup, pipe_LSTM.predict(X_tr_L)))\n",
    "    pred_LSTM_test = pipe_LSTM.predict(X_te_L)\n",
    "    # LSTM specific future data (requires warmup set built from the predictions on the test set\n",
    "    # or from the actual values, tbd)\n",
    "    # if from actual values, replace pred_LSTM_test with y_holdout in the call of LSTM_future\n",
    "    X_fut_L = data_prep.LSTM_future(X_future, pred_LSTM_test, X_te_L, window)\n",
    "    pred_LSTM_future = pipe_LSTM.predict(X_fut_L)\n",
    "    pred_LSTM_full = np.append(pred_LSTM_train, \n",
    "                              np.append(pred_LSTM_test, pred_LSTM_future))\n",
    "    \n",
    "    ## Gather the predictions and actual data into a single dataframe\n",
    "    well_data = data_prep.load_data(well)\n",
    "    full_predict = well_data[['date', 'avg_well_depth']].merge(all_dates, on='date', how='outer')\n",
    "    full_predict.rename(columns={'avg_well_depth':'Actual'}, inplace=True)\n",
    "    full_predict['Baseline'] = pred_avg_full\n",
    "    full_predict['Linear Reg'] = pred_lin_full\n",
    "    full_predict['CNN'] = pred_cnn_full\n",
    "    full_predict['LSTM'] = pred_LSTM_full\n",
    "    \n",
    "    ## Isolate the prediction on the holdout set for scoring\n",
    "    test_predict = full_predict.loc[full_predict['date']<=data_prep.get_end_date(well)][-365:].copy()\n",
    "    \n",
    "    ## Compute scores on the holdout set\n",
    "    RMSE_baseline = np.sqrt(np.mean((test_predict.Actual - test_predict.Baseline)**2))\n",
    "    MAE_baseline = np.mean(np.abs(test_predict.Actual - test_predict.Baseline))\n",
    "    \n",
    "    RMSE_lin_reg = np.sqrt(np.mean((test_predict.Actual - test_predict['Linear Reg'])**2))\n",
    "    MAE_lin_reg = np.mean(np.abs(test_predict.Actual - test_predict['Linear Reg']))\n",
    "    \n",
    "    RMSE_cnn = np.sqrt(np.mean((test_predict.Actual - test_predict.CNN)**2))\n",
    "    MAE_cnn = np.mean(np.abs(test_predict.Actual - test_predict.CNN))\n",
    "\n",
    "    RMSE_lstm = np.sqrt(np.mean((test_predict.Actual - test_predict.LSTM)**2))\n",
    "    MAE_lstm = np.mean(np.abs(test_predict.Actual - test_predict.LSTM))\n",
    "    \n",
    "    ## Update the scores in the dataframe\n",
    "    score_summary.RMSE = [RMSE_baseline, RMSE_lin_reg, RMSE_cnn, RMSE_lstm]\n",
    "    score_summary.MAE = [MAE_baseline, MAE_lin_reg, MAE_cnn, MAE_lstm]\n",
    "    \n",
    "    ## Save the results for this well in a dataframe\n",
    "    prediction_filename = 'model_predictions_'+well+'.pkl'\n",
    "    score_filename = 'model_scores_'+well+'.pkl'\n",
    "\n",
    "    full_predict.to_pickle(prediction_path+prediction_filename)\n",
    "    score_summary.to_pickle(score_path+score_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367460e8-fa2c-4f38-b2cd-1c801489c9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_AEK201 = pd.read_pickle('../data/prediction_pickles/model_predictions_AEK201.pkl')\n",
    "pred_AFL259 = pd.read_pickle('../data/prediction_pickles/model_predictions_AFL259.pkl')\n",
    "pred_APK309 = pd.read_pickle('../data/prediction_pickles/model_predictions_APK309.pkl')\n",
    "pred_APK310 = pd.read_pickle('../data/prediction_pickles/model_predictions_APK310.pkl')\n",
    "\n",
    "future_AEK201 = pd.read_csv('../data/raw_data/EIM-data-AEK201/EIMDiscreteResults_2023Oct22_39.csv')\n",
    "future_AEK201 = future_AEK201[['Result_Value','Field_Collection_End_Date']].copy()\n",
    "future_AEK201['date'] = pd.to_datetime(future_AEK201['Field_Collection_End_Date'])\n",
    "future_AEK201 = future_AEK201.drop('Field_Collection_End_Date', axis=1).copy()\n",
    "future_AEK201 = future_AEK201[['date','Result_Value']].sort_values('date').reset_index(drop=True)\n",
    "future_AEK201 = pred_AEK201[['date', 'Baseline', 'Linear Reg', 'CNN', 'LSTM']].merge(future_AEK201, on='date').copy()\n",
    "future_AEK201 = future_AEK201.rename(columns={'Result_Value': 'actual_depth'}).copy()\n",
    "future_AEK201 = future_AEK201.loc[future_AEK201.date > data_prep.get_end_date('AEK201')]\n",
    "\n",
    "future_AFL259 = pd.read_csv('../data/raw_data/EIM-data-AFL259/GroundwaterLevelsDiscreteResults_2023Nov12_24.csv')\n",
    "future_AFL259 = future_AFL259[['Water_Level_Value','Field_Collection_Date']].copy()\n",
    "future_AFL259['date'] = pd.to_datetime(future_AFL259['Field_Collection_Date'])\n",
    "future_AFL259 = future_AFL259.drop('Field_Collection_Date', axis=1).copy()\n",
    "future_AFL259 = future_AFL259[['date','Water_Level_Value']].sort_values('date').reset_index(drop=True)\n",
    "future_AFL259 = pred_AFL259[['date', 'Baseline', 'Linear Reg', 'CNN', 'LSTM']].merge(future_AFL259, on='date').copy()\n",
    "future_AFL259 = future_AFL259.rename(columns={'Water_Level_Value': 'actual_depth'}).copy()\n",
    "future_AFL259 = future_AFL259.loc[future_AFL259.date > data_prep.get_end_date('AFL259')]\n",
    "\n",
    "future_APK309 = pd.read_csv('../data/raw_data/EIM-data-APK309/GroundwaterLevelsDiscreteResults_2023Oct19_19.csv')\n",
    "future_APK309 = future_APK309[['Water_Level_Value','Field_Collection_Date']].copy()\n",
    "future_APK309['date'] = pd.to_datetime(future_APK309['Field_Collection_Date'])\n",
    "future_APK309 = future_APK309.drop('Field_Collection_Date', axis=1).copy()\n",
    "future_APK309 = future_APK309[['date','Water_Level_Value']].sort_values('date').reset_index(drop=True)\n",
    "future_APK309 = pred_APK309[['date', 'Baseline', 'Linear Reg', 'CNN', 'LSTM']].merge(future_APK309, on='date').copy()\n",
    "future_APK309 = future_APK309.rename(columns={'Water_Level_Value': 'actual_depth'}).copy()\n",
    "future_APK309 = future_APK309.loc[future_APK309.date > data_prep.get_end_date('APK309')]\n",
    "\n",
    "future_APK310 = pd.read_csv('../data/raw_data/EIM-data-APK310/GroundwaterLevelsDiscreteResults_2023Nov02_19.csv')\n",
    "future_APK310 = future_APK310[['Water_Level_Value','Field_Collection_Date']].copy()\n",
    "future_APK310['date'] = pd.to_datetime(future_APK310['Field_Collection_Date'])\n",
    "future_APK310 = future_APK310.drop('Field_Collection_Date', axis=1).copy()\n",
    "future_APK310 = future_APK310[['date','Water_Level_Value']].sort_values('date').reset_index(drop=True)\n",
    "future_APK310 = pred_APK310[['date', 'Baseline', 'Linear Reg', 'CNN', 'LSTM']].merge(future_APK310, on='date').copy()\n",
    "future_APK310 = future_APK310.rename(columns={'Water_Level_Value': 'actual_depth'}).copy()\n",
    "future_APK310 = future_APK310.loc[future_APK310.date > data_prep.get_end_date('APK310')]\n",
    "\n",
    "future_AEK201.to_pickle(future_pred_path+'future_data_compare_AEK201.pkl')\n",
    "future_AFL259.to_pickle(future_pred_path+'future_data_compare_AFL259.pkl')\n",
    "future_APK309.to_pickle(future_pred_path+'future_data_compare_APK309.pkl')\n",
    "future_APK310.to_pickle(future_pred_path+'future_data_compare_APK310.pkl')\n",
    "\n",
    "\n",
    "future_preds = [future_AEK201, future_AFL259, future_APK309, future_APK310]\n",
    "for well, pred in zip(wells, future_preds):\n",
    "    ## Compute scores on the holdout set\n",
    "    RMSE_baseline = np.sqrt(np.mean((pred.actual_depth - pred.Baseline)**2))\n",
    "    MAE_baseline = np.mean(np.abs(pred.actual_depth - pred.Baseline))\n",
    "    \n",
    "    RMSE_lin_reg = np.sqrt(np.mean((pred.actual_depth - pred.['Linear Reg'])**2))\n",
    "    MAE_lin_reg = np.mean(np.abs(pred.actual_depth - pred.['Linear Reg']))\n",
    "    \n",
    "    RMSE_cnn = np.sqrt(np.mean((pred.actual_depth - pred.CNN)**2))\n",
    "    MAE_cnn = np.mean(np.abs(pred.actual_depth - pred.CNN))\n",
    "\n",
    "    RMSE_lstm = np.sqrt(np.mean((pred.actual_depth - pred.LSTM)**2))\n",
    "    MAE_lstm = np.mean(np.abs(pred.actual_depth - pred.LSTM))\n",
    "    \n",
    "    ## Update the scores in the dataframe\n",
    "    score_summary.RMSE = [RMSE_baseline, RMSE_lin_reg, RMSE_cnn, RMSE_lstm]\n",
    "    score_summary.MAE = [MAE_baseline, MAE_lin_reg, MAE_cnn, MAE_lstm]\n",
    "    \n",
    "    ## Save the results for this well in a dataframe\n",
    "    score_filename = 'model_future_scores_'+well+'.pkl'\n",
    "    score_summary.to_pickle(score_path+score_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39600063-19a8-423c-9572-f0b2b0fb4592",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle('../data/prediction_pickles/model_scores_AEK201.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a70db9-4fa3-4f3d-aa16-36fb94b1634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle('../data/prediction_pickles/model_scores_AFL259.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4567c5-0e30-4369-836d-e9841fe22025",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle('../data/prediction_pickles/model_scores_APK309.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5164018f-a56e-4a0f-86c3-8b9bdb384670",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle('../data/prediction_pickles/model_scores_APK310.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64715363-014a-46ab-a010-c6213a794a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle('../data/prediction_pickles/model_future_scores_AEK201.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d7b39a-f4a2-413c-8999-575f75593589",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle('../data/prediction_pickles/model_future_scores_AFL259.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bafe49-484e-408b-93a7-e1eb3163894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle('../data/prediction_pickles/model_future_scores_APK309.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622c43f4-1a98-4fa1-96de-aaae60ab16fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle('../data/prediction_pickles/model_future_scores_APK310.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2656be-f5d0-4b51-821b-39fe7217f7f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
